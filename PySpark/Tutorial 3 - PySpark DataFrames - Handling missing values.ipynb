{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e53d6f7",
   "metadata": {},
   "source": [
    "## Pyspark handling missing values\n",
    "- dropping columns\n",
    "- dropping rows\n",
    "- various parameter in dropping functionalities\n",
    "- handling missing values by mean, median and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adfdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add3f657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name : string, age: int, experience: int, salary: int]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv('test3.csv', header=True, inferSchema=True)\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adea594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "|name | age|experience|salary|\n",
      "+-----+----+----------+------+\n",
      "|  ola|  25|         8| 10000|\n",
      "|kasia|  44|        10|  2000|\n",
      "| kuba|  55|         7|  4000|\n",
      "| bolo|  11|         8|  6000|\n",
      "|asdad|  22|      null| 33333|\n",
      "| gggg|  44|      null|  null|\n",
      "|   tt|null|      null|  null|\n",
      "|ddddd|  33|      null|  null|\n",
      "|  rrr|  22|        11| 33546|\n",
      "| null|  44|      null|  null|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c3ceca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| age|experience|salary|\n",
      "+----+----------+------+\n",
      "|  25|         8| 10000|\n",
      "|  44|        10|  2000|\n",
      "|  55|         7|  4000|\n",
      "|  11|         8|  6000|\n",
      "|  22|      null| 33333|\n",
      "|  44|      null|  null|\n",
      "|null|      null|  null|\n",
      "|  33|      null|  null|\n",
      "|  22|        11| 33546|\n",
      "|  44|      null|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop the columns\n",
    "df_pyspark.drop('name ').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21ff5b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "|name | age|experience|salary|\n",
      "+-----+----+----------+------+\n",
      "|  ola|  25|         8| 10000|\n",
      "|kasia|  44|        10|  2000|\n",
      "| kuba|  55|         7|  4000|\n",
      "| bolo|  11|         8|  6000|\n",
      "|asdad|  22|      null| 33333|\n",
      "| gggg|  44|      null|  null|\n",
      "|   tt|null|      null|  null|\n",
      "|ddddd|  33|      null|  null|\n",
      "|  rrr|  22|        11| 33546|\n",
      "| null|  44|      null|  null|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d649130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "|name |age|experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|  ola| 25|         8| 10000|\n",
      "|kasia| 44|        10|  2000|\n",
      "| kuba| 55|         7|  4000|\n",
      "| bolo| 11|         8|  6000|\n",
      "|  rrr| 22|        11| 33546|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## drop rows with nulls\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f7d0d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "|name |age|experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|  ola| 25|         8| 10000|\n",
      "|kasia| 44|        10|  2000|\n",
      "| kuba| 55|         7|  4000|\n",
      "| bolo| 11|         8|  6000|\n",
      "|  rrr| 22|        11| 33546|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### any == how\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7c2e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "|name |age|experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|  ola| 25|         8| 10000|\n",
      "|kasia| 44|        10|  2000|\n",
      "| kuba| 55|         7|  4000|\n",
      "| bolo| 11|         8|  6000|\n",
      "|asdad| 22|      null| 33333|\n",
      "|  rrr| 22|        11| 33546|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## threshold - keep rows with minimum thresh=num non nulls\n",
    "df_pyspark.na.drop(how='any',thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e00a6902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "|name |age|experience|salary|\n",
      "+-----+---+----------+------+\n",
      "|  ola| 25|         8| 10000|\n",
      "|kasia| 44|        10|  2000|\n",
      "| kuba| 55|         7|  4000|\n",
      "| bolo| 11|         8|  6000|\n",
      "|  rrr| 22|        11| 33546|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Subset - drop rows with null values in subset column\n",
    "df_pyspark.na.drop(how='any',subset=['experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96cfce52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|   name | age|experience|salary|\n",
      "+--------+----+----------+------+\n",
      "|     ola|  25|         8| 10000|\n",
      "|   kasia|  44|        10|  2000|\n",
      "|    kuba|  55|         7|  4000|\n",
      "|    bolo|  11|         8|  6000|\n",
      "|   asdad|  22|      null| 33333|\n",
      "|    gggg|  44|      null|  null|\n",
      "|      tt|null|      null|  null|\n",
      "|   ddddd|  33|      null|  null|\n",
      "|     rrr|  22|        11| 33546|\n",
      "|missing |  44|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Filling the missing values\n",
    "df_pyspark.na.fill('missing').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09b3743c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name : string, age: int, experience: int, salary: int]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "70d81b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name : string, age: float, experience: float, salary: float]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zamiana column int na float aby przeprowadzic funcje imputera \n",
    "from pyspark.sql.types import StringType\n",
    "df_pyspark2 = df_pyspark\n",
    "df_pyspark2 = df_pyspark2.withColumn(\"age\",df_pyspark[\"age\"].cast(FloatType()))\n",
    "df_pyspark2 = df_pyspark2.withColumn(\"salary\",df_pyspark[\"salary\"].cast(FloatType()))\n",
    "df_pyspark2 = df_pyspark2.withColumn(\"experience\",df_pyspark[\"experience\"].cast(FloatType()))\n",
    "# df_pyspark2 = df_pyspark.withColumn(\"age\",df_pyspark.age.cast('float'))\n",
    "# df_pyspark2 = df_pyspark.withColumn(\"experience\",df_pyspark.experience.cast('float'))\n",
    "# df_pyspark2 = df_pyspark.withColumn(\"salary\",df_pyspark.salary.cast('float'))\n",
    "\n",
    "df_pyspark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bfa641c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filling missing data with mean/median/etc.. values wtih imputer function from scikitlearn\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age','experience','salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age','experience','salary']]\n",
    "    ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "69f3a504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+-------+-----------+------------------+--------------+\n",
      "|name | age|experience| salary|age_imputed|experience_imputed|salary_imputed|\n",
      "+-----+----+----------+-------+-----------+------------------+--------------+\n",
      "|  ola|25.0|       8.0|10000.0|       25.0|               8.0|       10000.0|\n",
      "|kasia|44.0|      10.0| 2000.0|       44.0|              10.0|        2000.0|\n",
      "| kuba|55.0|       7.0| 4000.0|       55.0|               7.0|        4000.0|\n",
      "| bolo|11.0|       8.0| 6000.0|       11.0|               8.0|        6000.0|\n",
      "|asdad|22.0|      null|33333.0|       22.0|               8.8|       33333.0|\n",
      "| gggg|44.0|      null|   null|       44.0|               8.8|     14813.167|\n",
      "|   tt|null|      null|   null|  33.333332|               8.8|     14813.167|\n",
      "|ddddd|33.0|      null|   null|       33.0|               8.8|     14813.167|\n",
      "|  rrr|22.0|      11.0|33546.0|       22.0|              11.0|       33546.0|\n",
      "| null|44.0|      null|   null|       44.0|               8.8|     14813.167|\n",
      "+-----+----+----------+-------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark2).transform(df_pyspark2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321144e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
